{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethan Conn30 Apr at 16:00MealNotesHam and hummus sandwiches. Melon and kiwi.TeaAll PuddingAll Shamsa Mwechiwa                   Comment\n",
      "Ethan Conn30 Apr at 16:00MealNotesHam and hummus sandwiches. Melon and kiwi.TeaAll PuddingAll Shamsa Mwechiwa                   Comment\n",
      "Ethan Conn30 Apr at 16:00\n",
      "\n",
      "\n",
      "\n",
      "Ethan Conn30 Apr at 16:00\n",
      "Meal\n",
      "Meal\n",
      "NotesHam and hummus sandwiches. Melon and kiwi.\n",
      "Ham and hummus sandwiches. Melon and kiwi.\n",
      "TeaAll \n",
      "PuddingAll \n",
      "Shamsa Mwechiwa                   Comment\n",
      "Shamsa Mwechiwa                   \n",
      "Ethan Conn30 Apr at 11:30MealNotesMince beef dinner, mash potatoes and green beans. Tangerine. LunchMost PuddingAll Hoi Ping ToiComment\n",
      "Ethan Conn30 Apr at 11:30MealNotesMince beef dinner, mash potatoes and green beans. Tangerine. LunchMost PuddingAll Hoi Ping ToiComment\n",
      "Ethan Conn30 Apr at 11:30\n",
      "\n",
      "\n",
      "\n",
      "Ethan Conn30 Apr at 11:30\n",
      "Meal\n",
      "Meal\n",
      "NotesMince beef dinner, mash potatoes and green beans. Tangerine. \n",
      "Mince beef dinner, mash potatoes and green beans. Tangerine. \n",
      "LunchMost \n",
      "PuddingAll \n",
      "Hoi Ping ToiComment\n",
      "Hoi Ping Toi\n",
      "\n",
      "Ethan Conn29 Apr at 16:03MealNotesHummus/cream cheese/ salmon pinwheels \n",
      "Tinned apple.TeaAll PuddingAll Eleanor EastlandComment\n",
      "Ethan Conn29 Apr at 16:03MealNotesHummus/cream cheese/ salmon pinwheels \n",
      "Tinned apple.TeaAll PuddingAll Eleanor EastlandComment\n",
      "Ethan Conn29 Apr at 16:03\n",
      "\n",
      "\n",
      "\n",
      "Ethan Conn29 Apr at 16:03\n",
      "Meal\n",
      "Meal\n",
      "NotesHummus/cream cheese/ salmon pinwheels \n",
      "Tinned apple.\n",
      "Hummus/cream cheese/ salmon pinwheels \n",
      "Tinned apple.\n",
      "TeaAll \n",
      "PuddingAll \n",
      "Eleanor EastlandComment\n",
      "Eleanor Eastland\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_path = r\"C:\\Users\\andyc\\OneDrive\\Desktop\\repos\\parentzone\\Timeline _ ParentZone.html\"\n",
    "\n",
    "# Read HTML content from a file\n",
    "with open(html_path, 'r', encoding='utf-8') as file:\n",
    "    html_cont = file.read()\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(html_cont, 'html.parser')\n",
    "\n",
    "# Extract text from all tags\n",
    "divs_with_data_index = soup.find_all('div', attrs={'data-index': True})\n",
    "for div in divs_with_data_index:\n",
    "    # print(div.get_text())\n",
    "\n",
    "    # Extract text from child <div> tags\n",
    "    child_divs = div.find_all('div')\n",
    "    for child_div in child_divs:\n",
    "        print(child_div.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ham and hummus sandwiches. Melon and kiwi.\n",
      "Mince beef dinner, mash potatoes and green beans. Tangerine. \n",
      "Hummus/cream cheese/ salmon pinwheels \n",
      "Tinned apple.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "divs = soup.find_all('div', class_=\"content-notes\")\n",
    "for div in divs:\n",
    "    print(div.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Beanie pie topped with mashed potato and carrots \\nPineapple ',\n",
       " 'Orange ',\n",
       " 'Houmous and  Tuna pitta with tomato\\nStrawberry and raspberry ',\n",
       " 'Toast, blackberry and banana',\n",
       " 'Toast and plum',\n",
       " 'Lentil soup\\nCrackers with butter, grated cheese and apple']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "# options.add_argument('--headless')  # Run in headless mode (optional)\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Function to log in to the website\n",
    "def login_to_website(driver, url, username, password):\n",
    "    driver.get(url)\n",
    "   \n",
    "    # Find the login elements and perform login\n",
    "    username_input = driver.find_element(By.ID, 'email')  # Change 'username_field_id' to the actual ID\n",
    "    password_input = driver.find_element(By.ID, 'password')  # Change 'password_field_id' to the actual ID\n",
    "    login_button = driver.find_element(By.CSS_SELECTOR, \"[data-test-id='login_btn']\")      # Change 'login_button_id' to the actual ID\n",
    "\n",
    "    username_input.send_keys(username)\n",
    "    password_input.send_keys(password)\n",
    "    login_button.click()\n",
    "   \n",
    "    # Wait for the login to complete (you may need to adjust the wait time or use explicit waits)\n",
    "    time.sleep(2)\n",
    "\n",
    "# Function to scroll to the bottom of the page\n",
    "def scroll_to_bottom(driver):\n",
    "    text = []\n",
    "    # Find the scrollable div container\n",
    "    scroll_container = driver.find_element(By.CSS_SELECTOR, \"[data-test-id='timeline_scroll_container']\")\n",
    "    last_height = driver.execute_script(\"return arguments[0].scrollHeight;\", scroll_container)\n",
    "    # while True:\n",
    "    for _ in range(3):\n",
    "        \n",
    "        for div in driver.find_elements(By.CLASS_NAME, \"content-notes\"):\n",
    "            text.append(div.text)\n",
    "\n",
    "        # Scroll the scrollable div container\n",
    "        time.sleep(1)\n",
    "        driver.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight;\", scroll_container)\n",
    "        \n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return arguments[0].scrollHeight;\", scroll_container)\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "    return text\n",
    "\n",
    "def get_password():\n",
    "    with open('password.txt', 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Login credentials\n",
    "url = 'https://www.parentzone.me/login'\n",
    "username = 'andyconn1988@googlemail.com'\n",
    "password = get_password()\n",
    "\n",
    "# Log in to the website\n",
    "login_to_website(driver, url, username, password)\n",
    "\n",
    "# Navigate to the target page after login if necessary\n",
    "driver.get('https://www.parentzone.me/timeline')\n",
    "\n",
    "# Scroll to the bottom to load all data\n",
    "text = scroll_to_bottom(driver)\n",
    "text\n",
    "# # Get page source and parse it with BeautifulSoup\n",
    "# time.sleep(5)\n",
    "# # page_source = driver.page_source\n",
    "# page_source = driver.execute_script(\"return document.documentElement.outerHTML;\")\n",
    "# with open('soup_object.html', 'w', encoding='utf-8') as file:\n",
    "#     file.write(str(page_source))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Close the driver\n",
    "# driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "- Only a couple of items are shown at a time, they are stored in divs with  attribute 'data-index=\"1\"', 'data-index=\"2\"' etc. So need to scroll small amounts rather than to end of page. Get one item at a time, then scroll again until the next index item appears.\n",
    "\n",
    "As each item appears, store the information, then scroll again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cream cheese or houmous sandwiches \n",
      "Raisins \n",
      "Pasta salad\n",
      "Orange smiles \n",
      "Beef dinner with new potatoes and cabbage and Yorkshire pudding \n",
      "Pineapple \n"
     ]
    }
   ],
   "source": [
    "for div in driver.find_elements(By.CLASS_NAME, \"content-notes\"):\n",
    "    print(div.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "find() takes no keyword arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 15\u001b[0m\n\u001b[0;32m     10\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m div \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, attrs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata-index\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m248\u001b[39m}):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Extract the necessary information from each div\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     item \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;66;03m# 'title': div.find('div', class_='title-class').text,\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mdiv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdiv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcss-zc1z91-chip-meal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtext,\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent_notes\u001b[39m\u001b[38;5;124m'\u001b[39m: div\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent-notes\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext,\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;66;03m# Add more fields as necessary\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     }\n\u001b[0;32m     19\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend(item)\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m divChild \u001b[38;5;129;01min\u001b[39;00m div\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;31mTypeError\u001b[0m: find() takes no keyword arguments"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# Parse the html\n",
    "\n",
    "with open('soup_object.html', 'r', encoding='utf-8') as file:\n",
    "    soup_str = file.read()\n",
    "\n",
    "soup = BeautifulSoup(soup_str, 'html.parser')\n",
    "\n",
    "# Find and extract the desired information\n",
    "data = []\n",
    "for div in soup.find('div', attrs={\"data-index\": 248}):\n",
    "    # Extract the necessary information from each div\n",
    "    item = {\n",
    "        # 'title': div.find('div', class_='title-class').text,\n",
    "        'type': div.find('div', class_='css-zc1z91-chip-meal').text,\n",
    "        'content_notes': div.find('div', class_='content-notes').text,\n",
    "        # Add more fields as necessary\n",
    "    }\n",
    "    data.append(item)\n",
    "    for divChild in div.find('div'):\n",
    "        print(divChild.text, divChild)\n",
    "\n",
    "# Print or process the extracted data\n",
    "for item in data:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div data-index=\"248\" data-known-size=\"356\">\n",
       "<div class=\"css-1brpxzx-card\" data-test-id=\"post_container_145218124\">\n",
       "<div class=\"MuiPaper-root MuiPaper-elevation MuiPaper-rounded MuiPaper-elevation1 MuiCard-root css-s18byi\">\n",
       "<div class=\"MuiCardHeader-root css-faujvq\" data-test-id=\"post_145218124_children_wrapper\">\n",
       "<div class=\"MuiCardHeader-avatar css-1p83tvv\">\n",
       "<div class=\"css-1srb9fr-wrapper\" data-test-id=\"jest_user_avatar_wrapper\" style=\"position: inherit; border-color: transparent;\">\n",
       "<div class=\"MuiAvatar-root MuiAvatar-circular css-19lz4ip-medium\" data-test-id=\"jest_own_avatar\"><img alt=\"avatar of Ethan Conn\" class=\"MuiAvatar-img css-1hy9t21\" src=\"https://api.parentzone.me/v1/children/91151/avatar?key=d044ef74-8eba-490f-9efc-9332dabd737c\"/></div>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"MuiCardHeader-content css-11qjisw\"><span class=\"MuiTypography-root MuiTypography-body2 MuiCardHeader-title css-14tqbo1\"><span class=\"css-138uarv-bold\">Ethan Conn</span></span><span class=\"MuiTypography-root MuiTypography-body2 MuiCardHeader-subheader css-mbfek\"><span class=\"css-gvs35-black\">\n",
       "<p class=\"MuiTypography-root MuiTypography-body1 jss12 css-9l3uo3\">4 Mar\n",
       "                                                at 09:30</p>\n",
       "</span></span></div>\n",
       "</div>\n",
       "<div class=\"css-14mlsjc-tagContainer\" data-test-id=\"post_145218124_tags_wrapper\">\n",
       "<div class=\"MuiChip-root MuiChip-filled MuiChip-sizeMedium MuiChip-colorDefault MuiChip-filledDefault css-zc1z91-chip-meal\" data-test-id=\"post_145218124_tag_Meal\"><span class=\"MuiChip-icon MuiChip-iconMedium MuiChip-iconColorDefault css-1uumobq-icon\"><svg class=\"\" height=\"21\" style=\"display: inline-block; vertical-align: middle;\" viewbox=\"0 0 1024 1024\" width=\"21\" xmlns=\"http://www.w3.org/2000/svg\">\n",
       "<path d=\"M603.307 200.751c60.203-59.904 139.264-83.968 201.685-67.883l-110.037 110.037c-8.277 8.277-8.277 21.888 0 30.165 8.32 8.277 21.888 8.277 30.165 0l100.523-100.523c-0.085 0.213-0.171 0.427-0.299 0.683l7.936-7.893c8.277-8.277 21.845-8.277 30.165 0 7.936 7.979 8.021 20.651 0.725 29.013l-108.885 108.885c-8.277 8.277-8.277 21.888 0 30.165 8.32 8.277 21.888 8.277 30.165 0l111.488-111.445c18.219 63.147-5.931 144.213-67.029 205.355-67.883 67.84-159.616 89.173-224.896 58.88l-62.72 62.72 293.547 293.547-60.16 60.16-293.547-293.547-293.973 293.12-60.16-60.16 416.427-416.427c-30.293-65.28-8.96-157.056 58.88-224.853zM135.285 170.667l299.477 299.093-120.747 120.747-178.731-178.347c-66.603-66.987-66.603-174.933 0-241.493z\" style=\"fill: rgba(0, 0, 0, 0.87);\"></path>\n",
       "</svg></span><span class=\"MuiChip-label MuiChip-labelMedium css-9iedg7\">Meal</span></div>\n",
       "</div>\n",
       "<div class=\"MuiCardContent-root css-l7qgaw-cardContentOuter\">\n",
       "<h6 class=\"MuiTypography-root MuiTypography-subtitle2 css-som1hn-bold\">Notes\n",
       "                                        </h6>\n",
       "<div class=\"content-notes\">Toast, pear and banana</div>\n",
       "</div>\n",
       "<ul class=\"MuiList-root MuiList-padding css-1ontqvh\">\n",
       "<li class=\"MuiListItem-root MuiListItem-gutters MuiListItem-padding css-uswcmf\" style=\"margin-bottom: 0px;\">\n",
       "<div>\n",
       "<h6 class=\"MuiTypography-root MuiTypography-subtitle2 css-c7dfze\">AM Snack\n",
       "                                            </h6>\n",
       "<p class=\"MuiTypography-root MuiTypography-body2 css-1wdklh-breakWord\">All\n",
       "                                            </p>\n",
       "</div>\n",
       "</li>\n",
       "</ul>\n",
       "<div class=\"MuiCardActions-root MuiCardActions-spacing css-1tdk7i8-footer\">\n",
       "<div class=\"MuiChip-root MuiChip-outlined MuiChip-sizeMedium MuiChip-colorDefault MuiChip-outlinedDefault css-tg0bqq\" data-test-id=\"post_145218124_author_chip\">\n",
       "<div class=\"MuiAvatar-root MuiAvatar-circular MuiChip-avatar MuiChip-avatarMedium MuiChip-avatarColorDefault css-3i9vrz\">\n",
       "<img class=\"MuiAvatar-img css-1hy9t21\" src=\"https://api.parentzone.me/v1/staff/16275/avatar?key=d044ef74-8eba-490f-9efc-9332dabd737c\"/></div><span class=\"MuiChip-label MuiChip-labelMedium css-9iedg7\">Hoi Ping Toi</span>\n",
       "</div><a class=\"MuiButtonBase-root MuiButton-root MuiButton-text MuiButton-textPrimary MuiButton-sizeMedium MuiButton-textSizeMedium MuiButton-root MuiButton-text MuiButton-textPrimary MuiButton-sizeMedium MuiButton-textSizeMedium css-6gc19h-commentsButton-commentsButton\" data-test-id=\"post_145218124_comments_button\" href=\"/timeline/145218124/comments\" tabindex=\"0\">Comment<span class=\"MuiTouchRipple-root css-w0pj6f\"></span></a>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</div>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('div', attrs={\"data-index\": 248})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': '4 Mar\\n                                                at 09:30'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = {}\n",
    "output['date'] = soup.find('div', attrs={\"data-index\": 248}).find('p').text\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': '4 Mar\\n                                                at 09:30',\n",
       " 'content-notes': 'Toast, pear and banana',\n",
       " 'type': 'Meal',\n",
       " 'content_notes': 'Toast, pear and banana'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['content_notes'] = soup.find('div', attrs={\"data-index\": 248}).find('div', class_=\"content-notes\").text\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': '4 Mar\\n                                                at 09:30',\n",
       " 'content-notes': 'Toast, pear and banana',\n",
       " 'type': 'Meal'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['type'] = soup.find('div', attrs={\"data-index\": 248}).find('span', class_=\"css-9iedg7\").text\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': '4 Mar\\n                                                at 09:30',\n",
       " 'content-notes': 'Toast, pear and banana',\n",
       " 'type': 'Meal',\n",
       " 'content_notes': 'Toast, pear and banana',\n",
       " 'staff_member': <div class=\"MuiChip-root MuiChip-outlined MuiChip-sizeMedium MuiChip-colorDefault MuiChip-outlinedDefault css-tg0bqq\" data-test-id=\"post_145218124_author_chip\">\n",
       " <div class=\"MuiAvatar-root MuiAvatar-circular MuiChip-avatar MuiChip-avatarMedium MuiChip-avatarColorDefault css-3i9vrz\">\n",
       " <img class=\"MuiAvatar-img css-1hy9t21\" src=\"https://api.parentzone.me/v1/staff/16275/avatar?key=d044ef74-8eba-490f-9efc-9332dabd737c\"/></div><span class=\"MuiChip-label MuiChip-labelMedium css-9iedg7\">Hoi Ping Toi</span>\n",
       " </div>}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['staff_member'] = soup.find('div', attrs={\"data-index\": 248}).find('div', attrs={\"data-test-id\": \"post_145218124_author_chip\"})\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
