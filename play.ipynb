{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethan Conn30 Apr at 16:00MealNotesHam and hummus sandwiches. Melon and kiwi.TeaAll PuddingAll Shamsa Mwechiwa                   Comment\n",
      "Ethan Conn30 Apr at 16:00MealNotesHam and hummus sandwiches. Melon and kiwi.TeaAll PuddingAll Shamsa Mwechiwa                   Comment\n",
      "Ethan Conn30 Apr at 16:00\n",
      "\n",
      "\n",
      "\n",
      "Ethan Conn30 Apr at 16:00\n",
      "Meal\n",
      "Meal\n",
      "NotesHam and hummus sandwiches. Melon and kiwi.\n",
      "Ham and hummus sandwiches. Melon and kiwi.\n",
      "TeaAll \n",
      "PuddingAll \n",
      "Shamsa Mwechiwa                   Comment\n",
      "Shamsa Mwechiwa                   \n",
      "Ethan Conn30 Apr at 11:30MealNotesMince beef dinner, mash potatoes and green beans. Tangerine. LunchMost PuddingAll Hoi Ping ToiComment\n",
      "Ethan Conn30 Apr at 11:30MealNotesMince beef dinner, mash potatoes and green beans. Tangerine. LunchMost PuddingAll Hoi Ping ToiComment\n",
      "Ethan Conn30 Apr at 11:30\n",
      "\n",
      "\n",
      "\n",
      "Ethan Conn30 Apr at 11:30\n",
      "Meal\n",
      "Meal\n",
      "NotesMince beef dinner, mash potatoes and green beans. Tangerine. \n",
      "Mince beef dinner, mash potatoes and green beans. Tangerine. \n",
      "LunchMost \n",
      "PuddingAll \n",
      "Hoi Ping ToiComment\n",
      "Hoi Ping Toi\n",
      "\n",
      "Ethan Conn29 Apr at 16:03MealNotesHummus/cream cheese/ salmon pinwheels \n",
      "Tinned apple.TeaAll PuddingAll Eleanor EastlandComment\n",
      "Ethan Conn29 Apr at 16:03MealNotesHummus/cream cheese/ salmon pinwheels \n",
      "Tinned apple.TeaAll PuddingAll Eleanor EastlandComment\n",
      "Ethan Conn29 Apr at 16:03\n",
      "\n",
      "\n",
      "\n",
      "Ethan Conn29 Apr at 16:03\n",
      "Meal\n",
      "Meal\n",
      "NotesHummus/cream cheese/ salmon pinwheels \n",
      "Tinned apple.\n",
      "Hummus/cream cheese/ salmon pinwheels \n",
      "Tinned apple.\n",
      "TeaAll \n",
      "PuddingAll \n",
      "Eleanor EastlandComment\n",
      "Eleanor Eastland\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_path = r\"C:\\Users\\andyc\\OneDrive\\Desktop\\repos\\parentzone\\Timeline _ ParentZone.html\"\n",
    "\n",
    "# Read HTML content from a file\n",
    "with open(html_path, 'r', encoding='utf-8') as file:\n",
    "    html_cont = file.read()\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(html_cont, 'html.parser')\n",
    "\n",
    "# Extract text from all tags\n",
    "divs_with_data_index = soup.find_all('div', attrs={'data-index': True})\n",
    "for div in divs_with_data_index:\n",
    "    # print(div.get_text())\n",
    "\n",
    "    # Extract text from child <div> tags\n",
    "    child_divs = div.find_all('div')\n",
    "    for child_div in child_divs:\n",
    "        print(child_div.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ham and hummus sandwiches. Melon and kiwi.\n",
      "Mince beef dinner, mash potatoes and green beans. Tangerine. \n",
      "Hummus/cream cheese/ salmon pinwheels \n",
      "Tinned apple.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "divs = soup.find_all('div', class_=\"content-notes\")\n",
    "for div in divs:\n",
    "    print(div.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Beanie pie topped with mashed potato and carrots \\nPineapple ',\n",
       " 'Orange ',\n",
       " 'Houmous and  Tuna pitta with tomato\\nStrawberry and raspberry ',\n",
       " 'Toast, blackberry and banana',\n",
       " 'Toast and plum',\n",
       " 'Lentil soup\\nCrackers with butter, grated cheese and apple']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "# options.add_argument('--headless')  # Run in headless mode (optional)\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Function to log in to the website\n",
    "def login_to_website(driver, url, username, password):\n",
    "    driver.get(url)\n",
    "   \n",
    "    # Find the login elements and perform login\n",
    "    username_input = driver.find_element(By.ID, 'email')  # Change 'username_field_id' to the actual ID\n",
    "    password_input = driver.find_element(By.ID, 'password')  # Change 'password_field_id' to the actual ID\n",
    "    login_button = driver.find_element(By.CSS_SELECTOR, \"[data-test-id='login_btn']\")      # Change 'login_button_id' to the actual ID\n",
    "\n",
    "    username_input.send_keys(username)\n",
    "    password_input.send_keys(password)\n",
    "    login_button.click()\n",
    "   \n",
    "    # Wait for the login to complete (you may need to adjust the wait time or use explicit waits)\n",
    "    time.sleep(2)\n",
    "\n",
    "# Function to scroll to the bottom of the page\n",
    "def scroll_to_bottom(driver):\n",
    "    text = []\n",
    "    # Find the scrollable div container\n",
    "    scroll_container = driver.find_element(By.CSS_SELECTOR, \"[data-test-id='timeline_scroll_container']\")\n",
    "    last_height = driver.execute_script(\"return arguments[0].scrollHeight;\", scroll_container)\n",
    "    # while True:\n",
    "    for _ in range(3):\n",
    "        \n",
    "        for div in driver.find_elements(By.CLASS_NAME, \"content-notes\"):\n",
    "            text.append(div.text)\n",
    "\n",
    "        # Scroll the scrollable div container\n",
    "        time.sleep(1)\n",
    "        driver.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight;\", scroll_container)\n",
    "        \n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return arguments[0].scrollHeight;\", scroll_container)\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "    return text\n",
    "\n",
    "def get_password():\n",
    "    with open('password.txt', 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Login credentials\n",
    "url = 'https://www.parentzone.me/login'\n",
    "username = 'andyconn1988@googlemail.com'\n",
    "password = get_password()\n",
    "\n",
    "# Log in to the website\n",
    "login_to_website(driver, url, username, password)\n",
    "\n",
    "# Navigate to the target page after login if necessary\n",
    "driver.get('https://www.parentzone.me/timeline')\n",
    "\n",
    "# Scroll to the bottom to load all data\n",
    "text = scroll_to_bottom(driver)\n",
    "text\n",
    "# # Get page source and parse it with BeautifulSoup\n",
    "# time.sleep(5)\n",
    "# # page_source = driver.page_source\n",
    "# page_source = driver.execute_script(\"return document.documentElement.outerHTML;\")\n",
    "# with open('soup_object.html', 'w', encoding='utf-8') as file:\n",
    "#     file.write(str(page_source))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Close the driver\n",
    "# driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "- Only a couple of items are shown at a time, they are stored in divs with  attribute 'data-index=\"1\"', 'data-index=\"2\"' etc. So need to scroll small amounts rather than to end of page. Get one item at a time, then scroll again until the next index item appears.\n",
    "\n",
    "As each item appears, store the information, then scroll again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cream cheese or houmous sandwiches \n",
      "Raisins \n",
      "Pasta salad\n",
      "Orange smiles \n",
      "Beef dinner with new potatoes and cabbage and Yorkshire pudding \n",
      "Pineapple \n"
     ]
    }
   ],
   "source": [
    "for div in driver.find_elements(By.CLASS_NAME, \"content-notes\"):\n",
    "    print(div.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content_notes': None}\n",
      "{'content_notes': None}\n",
      "{'content_notes': 'Toast, pear and banana'}\n",
      "{'content_notes': None}\n",
      "{'content_notes': None}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parse the html\n",
    "\n",
    "with open('soup_object.html', 'r', encoding='utf-8') as file:\n",
    "    soup_str = file.read()\n",
    "\n",
    "soup = BeautifulSoup(soup_str, 'html.parser')\n",
    "\n",
    "# Find and extract the desired information\n",
    "data = []\n",
    "for div in soup.find('div', class_=\"MuiPaper-root\"):\n",
    "    # Extract the necessary information from each div\n",
    "    content_notes_div = div.find('div', class_='content-notes')\n",
    "    if content_notes_div:\n",
    "        content_notes = content_notes_div.text\n",
    "    else:\n",
    "        content_notes = None\n",
    "    item = {\n",
    "        # 'title': div.find('div', class_='title-class').text,\n",
    "        'content_notes': content_notes,\n",
    "        # Add more fields as necessary\n",
    "    }\n",
    "    data.append(item)\n",
    "\n",
    "# Print or process the extracted data\n",
    "for item in data:\n",
    "    print(item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
